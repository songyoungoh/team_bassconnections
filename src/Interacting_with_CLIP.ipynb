{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPHN7PJgKOzb"
      },
      "source": [
        "# Interacting with CLIP\n",
        "\n",
        "This is a self-contained notebook that shows how to download and run CLIP models, calculate the similarity between arbitrary image and text inputs, and perform zero-shot image classifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53N4k0pj_9qL"
      },
      "source": [
        "# Preparation for Colab\n",
        "\n",
        "Make sure you're running a GPU runtime; if not, select \"GPU\" as the hardware accelerator in Runtime > Change Runtime Type in the menu. The next cells will install the `clip` package and its dependencies, and check if PyTorch 1.7.1 or later is installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BpdJkdBssk9",
        "outputId": "4d9b51f8-d255-4868-97f6-be0a67dadfae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ftfy in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (6.1.3)\n",
            "Requirement already satisfied: regex in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (4.66.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-8np6zc7z\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-8np6zc7z\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: ftfy in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from clip==1.0) (6.1.3)\n",
            "Requirement already satisfied: regex in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /data/users/dl370/.local/lib/python3.8/site-packages (from clip==1.0) (2.0.1)\n",
            "Requirement already satisfied: torchvision in /data/users/dl370/.local/lib/python3.8/site-packages (from clip==1.0) (0.15.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from torch->clip==1.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /data/users/dl370/.local/lib/python3.8/site-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->clip==1.0) (68.2.2)\n",
            "Requirement already satisfied: wheel in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->clip==1.0) (0.41.2)\n",
            "Requirement already satisfied: cmake in /data/users/dl370/.local/lib/python3.8/site-packages (from triton==2.0.0->torch->clip==1.0) (3.28.1)\n",
            "Requirement already satisfied: lit in /data/users/dl370/.local/lib/python3.8/site-packages (from triton==2.0.0->torch->clip==1.0) (17.0.6)\n",
            "Requirement already satisfied: numpy in /data/users/dl370/.local/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from torchvision->clip==1.0) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /data/users/dl370/anaconda3/envs/clip/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /data/users/dl370/.local/lib/python3.8/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1hkDT38hSaP",
        "outputId": "70a44964-883d-4fd0-b95a-2c7f2b19aca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.0.1+cu117\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFxgLV5HAEEw"
      },
      "source": [
        "# Loading the model\n",
        "\n",
        "`clip.available_models()` will list the names of available CLIP models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLFS29hnhlY4",
        "outputId": "11779e1e-8bdd-4167-c18e-d26bdd6b67db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBRVTY9lbGm8",
        "outputId": "f06fd2fd-6126-475b-87d0-b10aa3b7da49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 151,277,313\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEVKsji6WOIX"
      },
      "source": [
        "## Building features\n",
        "\n",
        "We normalize the images, tokenize each text input, and run the forward pass of the model to get the image and text features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from data import data_create\n",
        "import yaml\n",
        "from torchvision import transforms, models, datasets\n",
        "with open('../config.yaml') as p:\n",
        "        config = yaml.safe_load(p)\n",
        "data_dir= config['data_dir']\n",
        "batch_size = 64\n",
        "dataloaders, dataset_sizes = data_create(data_dir, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [04:18<00:00,  1.05it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Dictionary to store embeddings and image paths\n",
        "embeddings = []\n",
        "images = []\n",
        "image_lables=[]\n",
        "for inputs, labels in tqdm(dataloaders['train']):\n",
        "    inputs = inputs.to('cuda')  # Move inputs to CUDA if using GPU\n",
        "    # print(inputs.shape)\n",
        "    \n",
        "    # Forward pass to get embeddings\n",
        "    with torch.no_grad():\n",
        "        features = model.encode_image(inputs).detach().cpu()\n",
        "\n",
        "    # Store embeddings\n",
        "    for i in range(inputs.size(0)):\n",
        "        images.append(inputs[i].cpu().numpy())\n",
        "        embeddings.append( features[i].numpy())\n",
        "        image_lables.append(labels[i].cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "\n",
        "def find_unique_closest_vectors(index_give, all_vecs):\n",
        "    \"\"\"\n",
        "    Find unique closest vectors from the set of all vectors to the intermediate points.\n",
        "\n",
        "    :param given_vec: The vector from which distances are measured.\n",
        "    :param all_vecs: A list of vectors to compare against the given vector.\n",
        "    :return: Six unique vectors from the set that are closest to the six evenly spaced points.\n",
        "    \"\"\"\n",
        "    # Calculate distances from the given vector to all other vectors\n",
        "    distances = [np.linalg.norm(all_vecs[index_give] - vec) for vec in all_vecs]\n",
        "\n",
        "    # Find the index of the maximum distance\n",
        "    max_distance_idx = np.argmax(distances)\n",
        "    print(max_distance_idx)\n",
        "\n",
        "    # Initialize set to keep track of chosen indices\n",
        "    chosen_indices = {max_distance_idx,index_give}\n",
        "\n",
        "    # Get the vector with the maximum distance\n",
        "    max_distance_vec = all_vecs[max_distance_idx]\n",
        "\n",
        "    # Generate six evenly spaced points between given_vec and max_distance_vec\n",
        "    step = (max_distance_vec - all_vecs[index_give]) / 7\n",
        "    intermediate_points = [all_vecs[index_give] + i * step for i in range(1, 7)]\n",
        "\n",
        "    # Find the nearest unique vector index in all_vecs for each intermediate point\n",
        "    closest_vector_indices = [index_give]  # Start with the max distance index\n",
        "    for point in intermediate_points:\n",
        "        min_distance = float('inf')\n",
        "        closest_idx = -1\n",
        "        for idx, vec in enumerate(all_vecs):\n",
        "            if idx not in chosen_indices:\n",
        "                distance = np.linalg.norm(point - vec)\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    closest_idx = idx\n",
        "        closest_vector_indices.append(closest_idx)\n",
        "        chosen_indices.add(closest_idx)\n",
        "    closest_vector_indices.append(max_distance_idx)\n",
        "\n",
        "    return closest_vector_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_img(index,allimages,alllables,label_to_class_mapping):\n",
        "    # Convert to (H, W, C) format and normalize\n",
        "    image_to_plot = np.transpose(allimages[index], (1, 2, 0))\n",
        "    image_to_plot = image_to_plot - image_to_plot.min()\n",
        "    image_to_plot = image_to_plot / image_to_plot.max()\n",
        "    num2labels= ['AnnualCrop', 'HerbaceousVegetation','Industrial','PermanentCrop',' River Forest ', 'Highway', 'Pasture ', 'Residential' , 'SeaLake']\n",
        "    # Plot the image\n",
        "    plt.imshow(image_to_plot)\n",
        "    plt.axis('off')  # Turn off axis numbers\n",
        "    plt.title(label_to_class_mapping[int(alllables[index])])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = dataloaders['train'].dataset.dataset.classes\n",
        "text_descriptions = [f\"This is a photo of a {label}\" for label in classes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/270 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 270/270 [00:24<00:00, 10.83it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Dictionary to store embeddings and image paths\n",
        "embeddings = []\n",
        "images = []\n",
        "image_lables=[]\n",
        "for inputs, labels in tqdm(dataloaders['train']):\n",
        "    inputs = inputs.to('cuda')  # Move inputs to CUDA if using GPU\n",
        "    # print(inputs.shape)\n",
        "    \n",
        "    # Forward pass to get embeddings\n",
        "    with torch.no_grad():\n",
        "        features = model.encode_image(inputs).detach().cpu()\n",
        "        features /= features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Store embeddings\n",
        "    for i in range(inputs.size(0)):\n",
        "        images.append(inputs[i].cpu().numpy())\n",
        "        embeddings.append( features[i].numpy())\n",
        "        image_lables.append(labels[i].cpu().numpy())\n",
        "\n",
        "text_descriptions = [f\"This is a photo of a {label}\" for label in dataloaders['train'].dataset.dataset.classes]\n",
        "text_tokens = clip.tokenize(text_descriptions).cuda()\n",
        "\n",
        "# Assuming embeddings is a list of numpy arrays\n",
        "embeddings_tensor = torch.tensor(embeddings)  # Convert list of numpy arrays to a PyTorch tensor\n",
        "\n",
        "# Check if CUDA is available and move the tensor to GPU\n",
        "if torch.cuda.is_available():\n",
        "    embeddings_tensor = embeddings_tensor.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "if text_features.dtype == torch.float32:\n",
        "    embeddings_tensor = embeddings_tensor.to(torch.float32)\n",
        "\n",
        "text_probs = (100.0 * embeddings_tensor @ text_features.T).softmax(dim=-1)\n",
        "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(8, 2 * len(images)))  # Adjust the figure size\n",
        "for i, image in enumerate(images):\n",
        "    plt.subplot(len(images), 1, i + 1)  # Arrange subplots in a single column\n",
        "    image_to_plot = np.transpose(image, (1, 2, 0))\n",
        "    plt.imshow(image_to_plot)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.subplots_adjust(hspace=0.5)  # Adjust horizontal spacing\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.33946759259259257"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'predicted' is the PyTorch tensor of model predictions and 'true_labels' is the list of numpy arrays\n",
        "\n",
        "# Convert the list of numpy arrays to a PyTorch tensor\n",
        "true_labels_tensor = torch.tensor([label.item() for label in image_lables])\n",
        "\n",
        "# Ensure that both tensors are on the same device - CPU in this case\n",
        "predicted = top_labels[:,0].cpu()\n",
        "true_labels_tensor = true_labels_tensor.cpu()\n",
        "\n",
        "# Calculate the number of correct predictions\n",
        "correct_predictions = torch.sum(predicted == true_labels_tensor)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = correct_predictions.item() / len(image_lables)\n",
        "\n",
        "accuracy  # This will be a value between 0 and 1, representing the proportion of correct predictions\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Interacting with CLIP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12e23e2819094ee0a079d4eb77cfc4f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1369964d45004b5e95a058910b2a33e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a5f52e56ede4ac3abe37a3ece007dc9",
              "IPY_MODEL_ce8b0faa1a1340b5a504d7b3546b3ccb"
            ],
            "layout": "IPY_MODEL_12e23e2819094ee0a079d4eb77cfc4f9"
          }
        },
        "161969cae25a49f38aacd1568d3cac6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a61c10fc00c4f04bb00b82e942da210": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e6adc4592124a4581b85f4c1f3bab4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "7a5f52e56ede4ac3abe37a3ece007dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a61c10fc00c4f04bb00b82e942da210",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e6adc4592124a4581b85f4c1f3bab4d",
            "value": 169001437
          }
        },
        "b597cd6f6cd443aba4bf4491ac7f957e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce8b0faa1a1340b5a504d7b3546b3ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_161969cae25a49f38aacd1568d3cac6c",
            "placeholder": "​",
            "style": "IPY_MODEL_b597cd6f6cd443aba4bf4491ac7f957e",
            "value": " 169001984/? [00:06&lt;00:00, 25734958.25it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
